{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 16:38:35.246497: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-27 16:38:35.246592: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import graph_nets as gn\n",
    "import sonnet as snt\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "#from pysr import pysr\n",
    "\n",
    "from simulator import read_orbits, base_classes\n",
    "from data.solar_system_names import *\n",
    "from ml_model_long import *\n",
    "\n",
    "sys.modules['base_classes'] = base_classes\n",
    "print('Started')\n",
    "\n",
    "tf.config.list_physical_devices('CPU')\n",
    "tf.config.run_functions_eagerly(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Global constants\n",
    "AU = 149.6e6 * 1000 # Astronomical Unit in meters.\n",
    "DAY = 24*3600. # Day in seconds\n",
    "YEAR = 365.25*DAY #Year\n",
    "delta_time = (0.5/24.) # 2 hours\n",
    "MSUN = 1.9885e+30\n",
    "MEARTH = 5.9724e+24\n",
    "G = 6.67428e-11/AU**3*MSUN*DAY**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables\n",
    "patience = 20 # For early stopping\n",
    "noise_level = 0.01 # Standard deviation of Gaussian noise for randomly perturbing input data\n",
    "num_epochs = 10000 # Number of training epochs. Set to large number\n",
    "num_time_steps_tr = 8000  # Number of time steps for training (~27 years).\n",
    "# One time step is 30 minutes\n",
    "# An orbit for saturn is 129110 steps\n",
    "num_time_steps_val = 80 # Using few to speed up calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_newton(x, m1, m2):\n",
    "    return G*m1*m2/np.linalg.norm(x, axis = -1, keepdims=True)**3.*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(num_time_steps_tr, num_time_steps_val):\n",
    "    \"\"\"\n",
    "    Read the data\n",
    "    :param num_time_steps_tr: Size of training set\n",
    "    :param num_time_steps_val: Size of validation set\n",
    "    :return: training data, validation data, and a Star System object\n",
    "    containing three dimensions: Time, body, and a length 6 dimension with\n",
    "    x and v\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the file\n",
    "    filename = './data/full.pkl'\n",
    "    filehandler = open(filename, 'rb')\n",
    "    system = pickle.load(filehandler)\n",
    "\n",
    "    # Extract the position and velocity\n",
    "    x = system.get_positions()\n",
    "    v = system.get_velocities()\n",
    "\n",
    "    # Concatenate them into a numpy array\n",
    "    # Data has three dimensions: Time, body, and a length 6 dimension with x and v\n",
    "    data = np.concatenate([x, v], axis=-1)\n",
    "\n",
    "    # Convert velocity into acceleration\n",
    "    A = data[1:, :, 3:] - data[:-1, :, 3:]\n",
    "    data[:-1, :, 3:] = A / delta_time\n",
    "    data = data[:-1]\n",
    "\n",
    "    # Eliminate unused data\n",
    "    data = data[:(num_time_steps_tr + num_time_steps_val)]\n",
    "\n",
    "    # Split into training and validation\n",
    "    data_tr = data[:num_time_steps_tr]\n",
    "    data_val = data[num_time_steps_tr:]\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.shuffle(data_tr)\n",
    "    np.random.shuffle(data_val)\n",
    "\n",
    "    return data_tr, data_val, system\n",
    "\n",
    "\n",
    "def format_data(data_tr, data_val, system):\n",
    "    \"\"\"\n",
    "    Convert the data into normalized tensorflow data objects that we can\n",
    "    use for training\n",
    "    \"\"\"\n",
    "\n",
    "    num_time_steps_tr = len(data_tr)\n",
    "\n",
    "    # Do not change this\n",
    "    batch_size_tr = 8\n",
    "    num_batches = num_time_steps_tr // batch_size_tr\n",
    "\n",
    "    nedges = system.numEdges\n",
    "\n",
    "    # Create empty arrays for the distances for training and validation\n",
    "    D_tr = np.empty([len(data_tr), nedges, 3])\n",
    "    D_val = np.empty([len(data_val), nedges, 3])\n",
    "\n",
    "    k = 0\n",
    "    # Create empty lists for the senders and receivers that will be used for\n",
    "    # the edges of the graph\n",
    "    senders, receivers = [], []\n",
    "    for i in range(system.numPlanets):\n",
    "        for j in range(system.numPlanets):\n",
    "            if i > j:\n",
    "                # For every pair of objects, assign a distance\n",
    "                D_tr[:, k, :] = data_tr[:, j, :3] - data_tr[:, i, :3]\n",
    "                D_val[:, k, :] = data_val[:, j, :3] - data_val[:, i, :3]\n",
    "\n",
    "                k += 1\n",
    "                # Add sender and receiver index\n",
    "                receivers.append(i)\n",
    "                senders.append(j)\n",
    "\n",
    "    # Accelerations\n",
    "    A_tr = data_tr[:, :, 3:]\n",
    "    A_val = data_val[:, :, 3:]\n",
    "    # Normalization of the accelerations\n",
    "    A_norm = G*1e-10 #np.std(A_tr)\n",
    "\n",
    "    # Flatten the arrays\n",
    "    D_tr = np.reshape(D_tr, [-1, 3])\n",
    "    D_val = np.reshape(D_val, [1, -1, 3])\n",
    "\n",
    "    A_tr = np.reshape(A_tr / A_norm, [-1, 3])\n",
    "    A_val = np.reshape(A_val / A_norm, [1, -1, 3])\n",
    "\n",
    "    # Convert them to tensors\n",
    "    D_tr = tf.convert_to_tensor(D_tr, dtype=\"float32\")\n",
    "    A_tr = tf.convert_to_tensor(A_tr, dtype=\"float32\")\n",
    "\n",
    "    D_val = tf.convert_to_tensor(D_val, dtype=\"float32\")\n",
    "    A_val = tf.convert_to_tensor(A_val, dtype=\"float32\")\n",
    "\n",
    "    # Split the training arrays into batches\n",
    "    D_tr_batches = tf.split(D_tr, num_batches)\n",
    "    A_tr_batches = tf.split(A_tr, num_batches)\n",
    "\n",
    "    # Convert into tensorflow dataset\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (D_tr_batches, A_tr_batches))\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (D_val, A_val))\n",
    "\n",
    "    # Create a normalization layer\n",
    "    norm_layer = Normalize_gn(cartesian_to_spherical_coordinates(D_tr))\n",
    "    return train_ds, test_ds, norm_layer, senders, receivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(data_val, system):\n",
    "    \"\"\"\n",
    "    Convert the data into normalized tensorflow data objects that we can\n",
    "    use for training\n",
    "    \"\"\"\n",
    "    nedges = system.numEdges\n",
    "\n",
    "    # Create empty arrays for the distances for training and validation\n",
    "    D_val = np.empty([10, nedges, 3])\n",
    "\n",
    "    k = 0\n",
    "    # Create empty lists for the senders and receivers that will be used for\n",
    "    # the edges of the graph\n",
    "    senders, receivers = [], []\n",
    "    for i in range(system.numPlanets):\n",
    "        for j in range(system.numPlanets):\n",
    "            if i > j:\n",
    "                # For every pair of objects, assign a distance\n",
    "                D_val[:, k, :] = data_val[:10, j, :3] - data_val[:10, i, :3]\n",
    "\n",
    "                k += 1\n",
    "\n",
    "    A_val = data_val[:10, :, 3:]\n",
    "    \n",
    "    return D_val, A_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d, a = check_data(data_val, system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G * masses[0] * masses[2] * d[:,1] / d[:,1]**3 / masses[2] / (G*1e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr, data_val, system = read_data(num_time_steps_tr, num_time_steps_val)\n",
    "train_ds, test_ds, norm_layer, senders, receivers = format_data(data_tr, data_val, system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = './saved_models/full'\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                            verbose = 1,\n",
    "                                            patience=patience,\n",
    "                                            #baseline = 0.1,\n",
    "                                            restore_best_weights=False)\n",
    "# Restore best weights not working, but found way around using checkpoint\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                 save_weights_only=False,\n",
    "                                                save_best_only=True,\n",
    "                                                 verbose=0)\n",
    "\n",
    "model = LearnForces(system.numPlanets, senders, receivers, norm_layer,\n",
    "                        noise_level=noise_level, masses=system.get_masses())\n",
    "\n",
    "#model.compile(run_eagerly=True)\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 16:39:04.961735: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-05-27 16:39:04.962218: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-27 16:39:05.650037: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 16:39:09.827532: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-05-27 16:39:12.709097: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 984/1000 [============================>.] - ETA: 1s - loss: inf"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, \n",
    "          epochs = num_epochs, \n",
    "          verbose=1, \n",
    "          callbacks=[early_stopping, checkpoint], \n",
    "          validation_data=test_ds\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./saved_models/full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LearnForces(system.numPlanets, senders, receivers, norm_layer,\n",
    "                        noise_level=noise_level, masses=system.get_masses())\n",
    "\n",
    "model2.compile()\n",
    "model2.evaluate(test_ds)\n",
    "#model.compile(run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights(\"./saved_models/tedt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mass, true_mass, name in zip(model.logm_planets.numpy(), system.get_masses(), system.get_names()):\n",
    "    print(name, np.round(np.log10(true_mass + 1e-22) + 10, 2), np.round(mass, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.load_model(\"./saved_models/orbits/reduced_weights.ckpt.data-00000-of-00001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "learned_masses = model.logm_planets.numpy() - model.logm_planets.numpy()[j]\n",
    "for i in range(nplanets):\n",
    "    print(f'{names[i]}, {np.log10(masses[i]/masses[j]):.2f}, {learned_masses[i]:.2f}, {abs(np.log10(masses[i]/masses[j]) -learned_masses[i])/abs(np.log10(masses[i])):.2f}')\n",
    "\n",
    "train_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap ,fp = model(D_val_flat[0], extract = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_val_new = np.empty([len(data_val), nedges, 3])\n",
    "k=0\n",
    "for i in range(nplanets):\n",
    "    for j in range(nplanets):\n",
    "        if i > j:\n",
    "            d_val = data_val[:,j,:3] - data_val[:,i,:3]\n",
    "            F_val_new[:,k,:] = force_newton(d_val, 10**learned_masses[i], 10**learned_masses[j]) #cartesian_to_spherical_coordinates(d_val)\n",
    "            k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = math.ceil(nplanets/4)\n",
    "fig, ax = plt.subplots(nrows, 4, figsize = (16, 4*nrows))\n",
    "for i in range(nplanets):\n",
    "    ax[i//4, i%4].set_title(names[i], fontsize = 16)\n",
    "    ax[i//4, i%4].plot(ap[:,i,0], ap[:,i,1], '.', label = 'Learned')\n",
    "    ax[i//4, i%4].plot(data_val[:,i,3]/A_norm, data_val[:,i,4]/A_norm, '.', label = 'Truth')\n",
    "\n",
    "ax[0,0].legend()\n",
    "#plt.savefig('/Users/Pablo/Desktop/full_learnedmasses.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = math.ceil(nedges/4)\n",
    "fig, ax = plt.subplots(nrows, 4, figsize = (16, 4*nrows))\n",
    "for i in range(nedges):\n",
    "    ax[i//4, i%4].set_title(names_edges[i])\n",
    "    #ax[i//4, i%4].plot(F_val_new[:,i,0], F_val_new[:,i,1], '.')\n",
    "    #ax[i//4, i%4].plot(fp[:,i,0]*n1/n2, fp[:,i,1]*n1/n2, '.')\n",
    "    ax[i//4, i%4].plot(fp[:,i,0], fp[:,i,1], '.')\n",
    "#plt.savefig('/Users/Pablo/Desktop/forces_learnedmasses.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = math.ceil(nedges/4)\n",
    "fig, ax = plt.subplots(nrows, 4, figsize = (16, 4*nrows))\n",
    "for i in range(nedges):\n",
    "    ax[i//4, i%4].set_title(names_edges[i])\n",
    "    ax[i//4, i%4].plot(F_val[:,i,0]*1e8, F_val[:,i,1]*1e8, '.')\n",
    "    #ax[i//4, i%4].plot(fp[:,i,0]*n1/n2, fp[:,i,1]*n1/n2, '.')\n",
    "    #ax[i//4, i%4].plot(fp[:,i,0], fp[:,i,1], '.')\n",
    "#plt.savefig('/Users/Pablo/Desktop/forces_learnedmasses.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros([nval, nedges,9])\n",
    "X[:,:,2:5] = D_val_np\n",
    "X[:,:,5] = np.linalg.norm(D_val_np, axis = -1)\n",
    "X[:,:,6:] = fp\n",
    "k=0\n",
    "for i in range(nplanets):\n",
    "    for j in range(nplanets):\n",
    "        if i > j:\n",
    "            X[:,k,0] = 10**(learned_masses[i])\n",
    "            X[:,k,1] = 10**(learned_masses[j])\n",
    "            #print(fp[0,k,0], X[0,k,0]*X[0,k,1]*X[0,k,2]/X[0,k,5]**3.)\n",
    "            k+=1 \n",
    "            \n",
    "N = 500\n",
    "ii = np.random.choice(nval, N, replace = False)\n",
    "X = X[ii]\n",
    "\n",
    "X[:,:,2:5], X[:,:,6:] = rotate_data(X[:,:,2:5], X[:,:,6:], uniform = False)\n",
    "\n",
    "X = np.reshape(X, [N*nedges,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    r = np.random.rand()\n",
    "    if r > 0.5:\n",
    "        X[i,0], X[i,1]  = X[i,1], X[i,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "norm_F = np.linalg.norm(X[:,6:], axis = -1)\n",
    "X_density = norm_F[:, None]\n",
    "bin_width = np.max(X_density)/30\n",
    "kde = KernelDensity(bandwidth=bin_width,\n",
    "                    kernel='cosine') #Want finite cutoff so not too expensive!\n",
    "kde.fit(X_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.linspace(0.0, 1.5e-3, num=1000)\n",
    "density_grid = np.exp(kde.score_samples(grid[:, None]))\n",
    "grid_idx = np.argmax((X_density < grid[1:]) & (X_density > grid[:-1]), axis=1)\n",
    "density_values = density_grid[grid_idx]\n",
    "inv_density = 1 / density_values\n",
    "inv_density /= inv_density.sum()\n",
    "np.random.seed(1)\n",
    "idx_for_pysr = np.random.choice(np.arange(len(X)), size=(1000,), p=inv_density)\n",
    "X_pysr = X[idx_for_pysr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X_pysr[:,6] #F_x\n",
    "#X[:, [0, 1]] = np.exp(X[:, [0, 1]])/1e23 #re-scale to prevent precision issues, since pysr uses 32-bit floats\n",
    "y /= np.std(y)                                 #same as above\n",
    "\n",
    "m_std = np.std(X_pysr[:,:2])\n",
    "x_std = np.std(X_pysr[:,2:5])\n",
    "X_pysr[:,:2]/= m_std\n",
    "X_pysr[:,2:]/= x_std\n",
    "\n",
    "kwargs = dict(populations=64, binary_operators=\"+ * - /\".split(\" \"),\n",
    "          unary_operators=[], temp_equation_file=True,\n",
    "          progress=False, procs=4, annealing=False, maxsize=40, useFrequency=True,  \n",
    "          variable_names = ['m0', 'm1', 'x', 'y', 'z', 'r'],\n",
    "          optimizer_algorithm=\"BFGS\",\n",
    "          optimizer_iterations=10,\n",
    "          optimize_probability=1.0,)\n",
    "equations=pysr(X_pysr[:,:6], y, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysr import best\n",
    "best(equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total time: ', str(datetime.timedelta(seconds=sr_time - start_time)))\n",
    "print('---------------')\n",
    "print('Time to set up data: ', str(datetime.timedelta(seconds=setup_time - start_time)))\n",
    "print('Time for training: ', str(datetime.timedelta(seconds=train_time - setup_time)))\n",
    "print('Time for plotting: ', str(datetime.timedelta(seconds=plot_time - train_time)))\n",
    "print('Time for symbolic regression: ', str(datetime.timedelta(seconds=sr_time - plot_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "### To finish the project\n",
    "- Working!\n",
    "\n",
    "### To clean the model\n",
    "- Improve plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "- The graphnets tensorflow 2 installation\n",
    "- pysr\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging (ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import tf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tr = 1\n",
    "\n",
    "i = 0\n",
    "while not np.isnan(loss_tr):\n",
    "    model = LearnForces(nplanets, senders, receivers, norm_layer, noise_level = noise_level)\n",
    "    model.compile(run_eagerly=True)\n",
    "\n",
    "    model.fit(train_ds, \n",
    "              verbose = 2,\n",
    "              epochs = 1,\n",
    "              steps_per_epoch = 10,\n",
    "              callbacks=[\n",
    "                    tf.keras.callbacks.TerminateOnNaN()]\n",
    "              #validation_data=test_ds\n",
    "             )\n",
    "    \n",
    "    loss_tr = tf_utils.to_numpy_or_python_type(model.evaluate(ini_ds, verbose = 0))\n",
    "    i+=1\n",
    "    if i>1000: \n",
    "        print('SUCCESS!')\n",
    "        break\n",
    "    #print(loss_tr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tr = np.NaN\n",
    "\n",
    "while np.isnan(loss_tr):\n",
    "    model = LearnForces(nplanets, senders, receivers, norm_layer, noise_level = noise_level)\n",
    "    model.compile(run_eagerly=True)\n",
    "\n",
    "    model.fit(train_ds, \n",
    "              verbose = 2,\n",
    "              epochs = 1,\n",
    "              steps_per_epoch = 10,\n",
    "              callbacks=[\n",
    "                    tf.keras.callbacks.TerminateOnNaN()]\n",
    "              #validation_data=test_ds\n",
    "             )\n",
    "    \n",
    "    loss_tr = tf_utils.to_numpy_or_python_type(model.evaluate(ini_ds, verbose = 0))\n",
    "    #print(loss_tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(gm):\n",
    "    return gm/6.67428e-11*1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{convert(324858.592) :.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:orbits]",
   "language": "python",
   "name": "conda-env-orbits-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
