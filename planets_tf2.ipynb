{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works really well! Tomorrow, first thing, is to upload to GitHub! \n",
    "\n",
    "Then, I will try to change my model to overriide the train_step and test_step functions as explained here: \n",
    "\n",
    "https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\n",
    "\n",
    "Then I can just run model.compile and model.fit! I can also easily add things like early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import graph_nets as gn\n",
    "import sonnet as snt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "DAY = 24*3600. # Day in seconds\n",
    "YEAR = 365.25*DAY #Year\n",
    "delta_time = (2/24.)*DAY/YEAR # 1 hour\n",
    "MSUN = 1.98892e+30\n",
    "MEARTH = 5.9742e+24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def shuffle_data(X):\n",
    "#    indices = np.random.permutation(np.arange(len(X)))\n",
    "#    Y = np.empty(X.shape)\n",
    "#    for i in range(len(X)):\n",
    "#        Y[i] = X[indices[i]]       \n",
    "#    return Y\n",
    "\n",
    "def reshape_senders_receivers(senders, receivers, batch_size, nplanets, nedges):\n",
    "    ''' Reshape receivers and senders to use in graph'''\n",
    "    x = np.arange(batch_size)\n",
    "    xx = x.reshape(batch_size,1)\n",
    "    y = np.ones(nedges)\n",
    "    z = np.reshape(xx+y-1, batch_size*nedges)*nplanets\n",
    "\n",
    "    senders = np.concatenate([senders]*batch_size) + z\n",
    "    receivers = np.concatenate([receivers]*batch_size) + z\n",
    "    \n",
    "    return senders, receivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables\n",
    "batch_size_tr = 75\n",
    "patience = 5\n",
    "d_patience = 0\n",
    "noise_level = 0.05\n",
    "log_every_iterations = 1000\n",
    "num_training_iterations = 200000\n",
    "\n",
    "# Do not change this\n",
    "total_time_traj = 20 #Years\n",
    "num_time_steps_total = int(total_time_traj/delta_time)\n",
    "num_time_steps_tr = 75000 \n",
    "#num_time_steps_val = int(total_time_traj/delta_time) - num_time_steps_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = np.array([MSUN/MEARTH, # Sun\n",
    "                   0.33011 * 10**24/MEARTH, # Mercury\n",
    "                   4.8685 * 10**24/MEARTH, # Venus\n",
    "                   1, # Earth\n",
    "                   0.642 * 10**24/MEARTH, #Mars\n",
    "                   1898.19 * 10**24/MEARTH, # Jupiter\n",
    "                   568 * 10**24/MEARTH, # Saturn\n",
    "          ])\n",
    "\n",
    "orbit_sun = np.loadtxt('nasa_orbits/sun_center/sun.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_mercury = np.loadtxt('nasa_orbits/sun_center/mercury.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_venus = np.loadtxt('nasa_orbits/sun_center/venus.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_earth = np.loadtxt('nasa_orbits/sun_center/earth.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_mars = np.loadtxt('nasa_orbits/sun_center/mars.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_jupiter = np.loadtxt('nasa_orbits/sun_center/jupiter.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_io = np.loadtxt('nasa_orbits/sun_center/io.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_europa = np.loadtxt('nasa_orbits/sun_center/europa.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_ganymede = np.loadtxt('nasa_orbits/sun_center/ganymede.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_callisto = np.loadtxt('nasa_orbits/sun_center/callisto.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_saturn = np.loadtxt('nasa_orbits/sun_center/saturn.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_dione = np.loadtxt('nasa_orbits/sun_center/dione.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_rhea = np.loadtxt('nasa_orbits/sun_center/rhea.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_titan = np.loadtxt('nasa_orbits/sun_center/titan.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "\n",
    "io_mass =0.08931900 * 10**24/MSUN\n",
    "europa_mass =0.048 * 10**24/MSUN\n",
    "ganymede_mass = 0.14819 * 10**24/MSUN\n",
    "callisto_mass =0.10759 * 10**24/MSUN\n",
    "\n",
    "dione_mass =0.0011 * 10**24/MSUN\n",
    "rhea_mass =0.0023 * 10**24/MSUN\n",
    "titan_mass =0.135 * 10**24/MSUN\n",
    "\n",
    "orbit_jupiter[:,:] += (orbit_io[:,:]*io_mass + orbit_europa[:,:]*europa_mass + \n",
    "                       orbit_ganymede[:,:]*ganymede_mass + orbit_callisto[:,:]*callisto_mass)/masses[5]\n",
    "\n",
    "\n",
    "orbit_saturn[:,:] += (orbit_dione[:,:]*dione_mass + orbit_rhea[:,:]*rhea_mass + \n",
    "                       orbit_titan[:,:]*titan_mass)/masses[6]\n",
    "\n",
    "nplanets = len(masses)\n",
    "nedges = nplanets*(nplanets-1)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.stack([orbit_sun, \n",
    "                 orbit_mercury, \n",
    "                 orbit_venus,\n",
    "                 orbit_earth, \n",
    "                 orbit_mars, \n",
    "                 orbit_jupiter, \n",
    "                 orbit_saturn\n",
    "                ])\n",
    "data = data.transpose(2,0,1)\n",
    "\n",
    "# Change frame of reference to the barycenter of the planets we are using\n",
    "P = masses[np.newaxis, :, np.newaxis]*data[:,:,3:]\n",
    "V_ref = np.sum(P, axis = 1,keepdims=True)/np.sum(masses)\n",
    "data[:,:,3:] -= V_ref\n",
    "\n",
    "A = data[1:,:,3:] - data[:-1,:,3:]\n",
    "data[:-1, :, 3:] = A \n",
    "data = data[:-1]\n",
    "\n",
    "# Split into training and validation\n",
    "data_tr = data[:num_time_steps_tr]\n",
    "data_val = data[num_time_steps_tr:]\n",
    "\n",
    "num_time_steps_val = len(data_val)\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data_tr)\n",
    "np.random.shuffle(data_val)\n",
    "\n",
    "D_tr = np.empty([len(data_tr), nedges, 3])\n",
    "D_val = np.empty([len(data_val), nedges, 3])\n",
    "k=0\n",
    "senders, receivers = [], []\n",
    "for i in range(nplanets):\n",
    "    for j in range(nplanets):\n",
    "        if i > j:\n",
    "            d_tr = data_tr[:,j,:3] - data_tr[:,i,:3]\n",
    "            d_val = data_val[:,j,:3] - data_val[:,i,:3]\n",
    "            D_tr[:,k,:] = d_tr#cartesian_to_spherical_coordinates(d_tr)\n",
    "            D_val[:,k,:] = d_val #cartesian_to_spherical_coordinates(d_val)\n",
    "            \n",
    "            k+=1 \n",
    "            receivers.append(i)\n",
    "            senders.append(j)\n",
    "\n",
    "A_tr = data_tr[:,:,3:]\n",
    "A_val = data_val[:,:,3:]\n",
    "A_norm =np.std(A_tr) \n",
    "\n",
    "D_tr_flat = np.reshape(D_tr, [num_time_steps_tr*nedges, 3])\n",
    "D_val_flat = np.reshape(D_val,[1, num_time_steps_val*nedges, 3])\n",
    "\n",
    "A_tr_flat = np.reshape(A_tr/A_norm, [num_time_steps_tr*nplanets, 3])\n",
    "A_val_flat = np.reshape(A_val/A_norm, [1, num_time_steps_val*nplanets, 3])\n",
    "\n",
    "D_tr = tf.convert_to_tensor(D_tr_flat, dtype=\"float32\")\n",
    "A_tr = tf.convert_to_tensor(A_tr_flat, dtype=\"float32\")\n",
    "D_tr_batches = tf.split(D_tr,  1000)\n",
    "A_tr_batches = tf.split(A_tr,  1000)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (D_tr_batches, A_tr_batches))\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (D_val_flat, A_val_flat))\n",
    "\n",
    "#D_val = tf.convert_to_tensor(D_val[:,:,:], dtype=\"float32\")\n",
    "#A_val = tf.convert_to_tensor(A_val/A_norm, dtype=\"float32\")\n",
    "\n",
    "#D_val_flat = tf.reshape(D_val, shape = [num_time_steps_val*nedges, 3])\n",
    "#A_val_flat = tf.reshape(A_val/A_norm, shape = [num_time_steps_val*nedges, 3])\n",
    "\n",
    "nodes_tr = np.concatenate([np.log(masses)]*batch_size_tr)[:,np.newaxis]\n",
    "nodes_val = np.concatenate([np.log(masses)]*num_time_steps_val)[:,np.newaxis]\n",
    "\n",
    "senders_tr, receivers_tr = reshape_senders_receivers(senders, receivers, batch_size_tr, nplanets, nedges)\n",
    "senders_val, receivers_val = reshape_senders_receivers(senders, receivers, num_time_steps_val, nplanets, nedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1575000, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nplanets, _, ntime = X.shape\n",
    "loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "loss_test = tf.keras.metrics.MeanAbsoluteError(name='loss_test')\n",
    "\n",
    "class LearnForces(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LearnForces, self).__init__()\n",
    "        self.test_loss_metric = tf.keras.metrics.MeanAbsoluteError(name='test_loss')\n",
    "        self.nplanets = nplanets\n",
    "        \n",
    "        #m_init = tf.random_normal_initializer()\n",
    "        #self.m = tf.Variable(\n",
    "        #    initial_value=m_init(shape=(self.nplanets,), dtype=\"float32\"),\n",
    "        #    trainable=True,\n",
    "        #)\n",
    "        \n",
    "        self.graph_network = gn.blocks.EdgeBlock(\n",
    "            #edge_model_fn=lambda: snt.Linear(output_size=3),\n",
    "            edge_model_fn=lambda: snt.nets.MLP([32, 32, 3]), \n",
    "            use_edges = True,\n",
    "            use_receiver_nodes = True,\n",
    "            use_sender_nodes = True,\n",
    "            use_globals = False,\n",
    "        )\n",
    "\n",
    "    def sum_forces(self, graph):\n",
    "        b1_tr = gn.blocks.ReceivedEdgesToNodesAggregator(reducer = tf.math.unsorted_segment_sum)(graph)\n",
    "        b2_tr = gn.blocks.SentEdgesToNodesAggregator(reducer = tf.math.unsorted_segment_sum)(graph)\n",
    "        summed_forces = b1_tr-b2_tr\n",
    "        return summed_forces\n",
    "            \n",
    "    def get_acceleration(self, forces, graph):\n",
    "        acceleration_tr = tf.divide(forces, tf.exp(graph.nodes))\n",
    "        return acceleration_tr\n",
    "        #output_ops_tr = tf.reshape(acceleration_tr, shape=[self.ntime, self.nplanets, 3])\n",
    "        #return output_ops_tr\n",
    "        \n",
    "    def call(self, g):\n",
    "        #self.ntime = len(g.nodes)//nplanets\n",
    "        g = self.graph_network(g)\n",
    "        f = self.sum_forces(g)\n",
    "        a = self.get_acceleration(f, g)\n",
    "        return a\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # Unpack the data\n",
    "        D, A = data\n",
    "        #print(len(D))\n",
    "        \n",
    "        # Could make a \"get_senders_receivers\" function, that takes\n",
    "        # nplanets and returns the arrays. \n",
    "        # Will also have to make one that gets the nodes from the \n",
    "        # masses it is learning\n",
    "        # That will make everything more self contained\n",
    "\n",
    "        graph_dict = { \n",
    "          \"nodes\": nodes_tr,\n",
    "          \"edges\": D, \n",
    "          \"receivers\": receivers_tr, \n",
    "          \"senders\": senders_tr \n",
    "           } \n",
    "    \n",
    "        g = gn.utils_tf.data_dicts_to_graphs_tuple([graph_dict])\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = self(g)\n",
    "            # Compute the loss\n",
    "            loss = tf.keras.losses.mean_squared_error(A, predictions)\n",
    "        \n",
    "        # Compute gradients\n",
    "        trainable_vars = model.graph_network.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients,trainable_vars))\n",
    "\n",
    "        loss_tracker.update_state(loss)\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    def test_step(self,data):\n",
    "        # Unpack the data\n",
    "        D, A = data\n",
    "        \n",
    "        graph_dict = { \n",
    "          \"nodes\": nodes_val,\n",
    "          \"edges\": D, \n",
    "          \"receivers\": receivers_val,\n",
    "          \"senders\": senders_val \n",
    "           } \n",
    "\n",
    "        g = gn.utils_tf.data_dicts_to_graphs_tuple([graph_dict])\n",
    "        \n",
    "        predictions = self(g)\n",
    "\n",
    "        #self.A_pred = tf.reshape(predictions, shape=[-1, self.nplanets, 3])\n",
    "        #loss_test = tf.keras.metrics.MeanAbsoluteError(A, predictions)\n",
    "        #loss_tracker.update_state(loss_test)\n",
    "\n",
    "        #self.losses.append(tf.keras.losses.MeanAbsoluteError)\n",
    "        # Updates the metrics tracking the loss\n",
    "        #self.compiled_loss(A, predictions, regularization_losses=self.losses) \n",
    "    \n",
    "        # Update the metrics.\n",
    "        loss_test.update_state(A, predictions)\n",
    "        \n",
    "        return {\"loss\": loss_test.result()}\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker, loss_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "model = LearnForces()\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 7s 1ms/step - loss: 4.1831\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7668\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4584\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3061\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2139\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1450\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1002\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0778\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0633\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0515\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0438\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0369\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0309\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0270\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0239\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0212\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0189\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0168\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0148\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0135\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0123\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0113\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0103\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0093\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0085\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0079\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0074\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0069\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0065\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0062\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0059\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0056\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0054\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0052\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0050\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0048\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0046\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0044\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0042\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0040\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0039\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0038\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0037\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0036\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0035\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0035\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0034\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0033\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0033\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0032\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0031\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0031\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0030A: 0s\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0030\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0029\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0029\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0028\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0028\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0028\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0027\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0027\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0027\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0026\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0026\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0026\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0025\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0025\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0025\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0024\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0024\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0024\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0024\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0023\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0023\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0023\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0023\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0023\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0022\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0022\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0022\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0022\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0022\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0022\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0021\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0021\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0021\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0021\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0021\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0021\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0021\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0020\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0020\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0020\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0020\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0020\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0019\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0019\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0019\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0019\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc9e9830b10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs = 100, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.0266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.026570908725261688"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = { \n",
    "          \"nodes\": nodes_val,\n",
    "          \"edges\": D_val_flat[0], \n",
    "          \"receivers\": receivers_val, \n",
    "          \"senders\": senders_val \n",
    "           } \n",
    "    \n",
    "g = gn.utils_tf.data_dicts_to_graphs_tuple([graph_dict])\n",
    "out = model(g)\n",
    "ap = tf.reshape(out, shape=[-1, nplanets, 3]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9ccb08550>,\n",
       " <matplotlib.lines.Line2D at 0x7fc9ce321c10>,\n",
       " <matplotlib.lines.Line2D at 0x7fc9cc1361d0>,\n",
       " <matplotlib.lines.Line2D at 0x7fc9cc136890>,\n",
       " <matplotlib.lines.Line2D at 0x7fc9cda7fad0>,\n",
       " <matplotlib.lines.Line2D at 0x7fc9cdba42d0>,\n",
       " <matplotlib.lines.Line2D at 0x7fc9cdba4310>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ap[:,:,0], ap[:,:,1], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "### To finish the project\n",
    "- Include symbolic regression\n",
    "- Include moons\n",
    "- Learn masses\n",
    "- Extract forces\n",
    "- Add 3D rotations?\n",
    "- Add noise?\n",
    "- Add custom loss function?\n",
    "\n",
    "### To clean the model\n",
    "- Include loss_test in fit \n",
    "- Use it for early stopping? \n",
    "- Make the model calculate the senders and receivers from input shape\n",
    "- Include creating the graph in the model call, so the model takes an array of distances, not a graph? (Check if this makes things slower). See https://www.tensorflow.org/guide/keras/functional\n",
    "- Make a separate script that creates the data\n",
    "- Include some kind of training progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gn_tf2",
   "language": "python",
   "name": "gn_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
