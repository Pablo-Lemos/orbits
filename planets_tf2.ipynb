{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works really well! Tomorrow, first thing, is to upload to GitHub! \n",
    "\n",
    "Then, I will try to change my model to overriide the train_step and test_step functions as explained here: \n",
    "\n",
    "https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\n",
    "\n",
    "Then I can just run model.compile and model.fit! I can also easily add things like early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import graph_nets as gn\n",
    "import sonnet as snt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "DAY = 24*3600. # Day in seconds\n",
    "YEAR = 365.25*DAY #Year\n",
    "delta_time = (2/24.)*DAY/YEAR # 1 hour\n",
    "MSUN = 1.98892e+30\n",
    "MEARTH = 5.9742e+24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def shuffle_data(X):\n",
    "#    indices = np.random.permutation(np.arange(len(X)))\n",
    "#    Y = np.empty(X.shape)\n",
    "#    for i in range(len(X)):\n",
    "#        Y[i] = X[indices[i]]       \n",
    "#    return Y\n",
    "\n",
    "def reshape_senders_receivers(senders, receivers, batch_size, nplanets, nedges):\n",
    "    ''' Reshape receivers and senders to use in graph'''\n",
    "    x = np.arange(batch_size)\n",
    "    xx = x.reshape(batch_size,1)\n",
    "    y = np.ones(nedges)\n",
    "    z = np.reshape(xx+y-1, batch_size*nedges)*nplanets\n",
    "\n",
    "    senders = np.concatenate([senders]*batch_size) + z\n",
    "    receivers = np.concatenate([receivers]*batch_size) + z\n",
    "    \n",
    "    return senders, receivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables\n",
    "batch_size_tr = 75\n",
    "patience = 5\n",
    "d_patience = 0\n",
    "noise_level = 0.05\n",
    "log_every_iterations = 1000\n",
    "num_training_iterations = 200000\n",
    "\n",
    "# Do not change this\n",
    "total_time_traj = 20 #Years\n",
    "num_time_steps_total = int(total_time_traj/delta_time)\n",
    "num_time_steps_tr = 75000 \n",
    "#num_time_steps_val = int(total_time_traj/delta_time) - num_time_steps_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbit_sun = np.loadtxt('nasa_orbits/sun_center/sun.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_mercury = np.loadtxt('nasa_orbits/sun_center/mercury.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')\n",
    "orbit_venus = np.loadtxt('nasa_orbits/sun_center/venus.txt', usecols = [2,3,4, 5, 6, 7], unpack=True, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = np.array([MSUN/MEARTH, \n",
    "          0.33011 * 10**24/MEARTH,\n",
    "          4.8685 * 10**24/MEARTH\n",
    "          ])\n",
    "\n",
    "nplanets = len(masses)\n",
    "nedges = nplanets*(nplanets-1)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.stack([orbit_sun, \n",
    "                 orbit_mercury, \n",
    "                 orbit_venus,\n",
    "                ])\n",
    "data = data.transpose(2,0,1)\n",
    "\n",
    "# Change frame of reference to the barycenter of the planets we are using\n",
    "P = masses[np.newaxis, :, np.newaxis]*data[:,:,3:]\n",
    "V_ref = np.sum(P, axis = 1,keepdims=True)/np.sum(masses)\n",
    "data[:,:,3:] -= V_ref\n",
    "\n",
    "A = data[1:,:,3:] - data[:-1,:,3:]\n",
    "data[:-1, :, 3:] = A \n",
    "data = data[:-1]\n",
    "\n",
    "# Split into training and validation\n",
    "data_tr = data[:num_time_steps_tr]\n",
    "data_val = data[num_time_steps_tr:]\n",
    "\n",
    "num_time_steps_val = len(data_val)\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data_tr)\n",
    "np.random.shuffle(data_val)\n",
    "\n",
    "D_tr = np.empty([len(data_tr), nedges, 3])\n",
    "D_val = np.empty([len(data_val), nedges, 3])\n",
    "k=0\n",
    "senders, receivers = [], []\n",
    "for i in range(nplanets):\n",
    "    for j in range(nplanets):\n",
    "        if i > j:\n",
    "            d_tr = data_tr[:,j,:3] - data_tr[:,i,:3]\n",
    "            d_val = data_val[:,j,:3] - data_val[:,i,:3]\n",
    "            D_tr[:,k,:] = d_tr#cartesian_to_spherical_coordinates(d_tr)\n",
    "            D_val[:,k,:] = d_val #cartesian_to_spherical_coordinates(d_val)\n",
    "            \n",
    "            k+=1 \n",
    "            receivers.append(i)\n",
    "            senders.append(j)\n",
    "\n",
    "A_tr = data_tr[:,:,3:]\n",
    "A_val = data_val[:,:,3:]\n",
    "A_norm =np.std(A_tr) \n",
    "\n",
    "D_tr_flat = np.reshape(D_tr, [num_time_steps_tr*nedges, 3])\n",
    "D_val_flat = np.reshape(D_val,[1, num_time_steps_val*nedges, 3])\n",
    "\n",
    "A_tr_flat = np.reshape(A_tr/A_norm, [num_time_steps_tr*nedges, 3])\n",
    "A_val_flat = np.reshape(A_val/A_norm, [1, num_time_steps_val*nedges, 3])\n",
    "\n",
    "D_tr = tf.convert_to_tensor(D_tr_flat, dtype=\"float32\")\n",
    "A_tr = tf.convert_to_tensor(A_tr_flat, dtype=\"float32\")\n",
    "D_tr_batches = tf.split(D_tr,  1000)\n",
    "A_tr_batches = tf.split(A_tr,  1000)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (D_tr_batches, A_tr_batches))\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (D_val_flat, A_val_flat))\n",
    "\n",
    "#D_val = tf.convert_to_tensor(D_val[:,:,:], dtype=\"float32\")\n",
    "#A_val = tf.convert_to_tensor(A_val/A_norm, dtype=\"float32\")\n",
    "\n",
    "#D_val_flat = tf.reshape(D_val, shape = [num_time_steps_val*nedges, 3])\n",
    "#A_val_flat = tf.reshape(A_val/A_norm, shape = [num_time_steps_val*nedges, 3])\n",
    "\n",
    "nodes_tr = np.concatenate([np.log(masses)]*batch_size_tr)[:,np.newaxis]\n",
    "nodes_val = np.concatenate([np.log(masses)]*num_time_steps_val)[:,np.newaxis]\n",
    "\n",
    "senders_tr, receivers_tr = reshape_senders_receivers(senders, receivers, batch_size_tr, nplanets, nedges)\n",
    "senders_val, receivers_val = reshape_senders_receivers(senders, receivers, num_time_steps_val, nplanets, nedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nplanets, _, ntime = X.shape\n",
    "loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "loss_test = tf.keras.metrics.MeanAbsoluteError(name='loss_test')\n",
    "\n",
    "class LearnForces(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LearnForces, self).__init__()\n",
    "        self.test_loss_metric = tf.keras.metrics.MeanAbsoluteError(name='test_loss')\n",
    "        self.nplanets = nplanets\n",
    "        \n",
    "        #m_init = tf.random_normal_initializer()\n",
    "        #self.m = tf.Variable(\n",
    "        #    initial_value=m_init(shape=(self.nplanets,), dtype=\"float32\"),\n",
    "        #    trainable=True,\n",
    "        #)\n",
    "        \n",
    "        self.graph_network = gn.blocks.EdgeBlock(\n",
    "            #edge_model_fn=lambda: snt.Linear(output_size=3),\n",
    "            edge_model_fn=lambda: snt.nets.MLP([32, 32, 3]), \n",
    "            use_edges = True,\n",
    "            use_receiver_nodes = True,\n",
    "            use_sender_nodes = True,\n",
    "            use_globals = False,\n",
    "        )\n",
    "\n",
    "    def sum_forces(self, graph):\n",
    "        b1_tr = gn.blocks.ReceivedEdgesToNodesAggregator(reducer = tf.math.unsorted_segment_sum)(graph)\n",
    "        b2_tr = gn.blocks.SentEdgesToNodesAggregator(reducer = tf.math.unsorted_segment_sum)(graph)\n",
    "        summed_forces = b1_tr-b2_tr\n",
    "        return summed_forces\n",
    "            \n",
    "    def get_acceleration(self, forces, graph):\n",
    "        acceleration_tr = tf.divide(forces, tf.exp(graph.nodes))\n",
    "        return acceleration_tr\n",
    "        #output_ops_tr = tf.reshape(acceleration_tr, shape=[self.ntime, self.nplanets, 3])\n",
    "        #return output_ops_tr\n",
    "        \n",
    "    def call(self, g):\n",
    "        #self.ntime = len(g.nodes)//nplanets\n",
    "        g = self.graph_network(g)\n",
    "        f = self.sum_forces(g)\n",
    "        a = self.get_acceleration(f, g)\n",
    "        return a\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # Unpack the data\n",
    "        D, A = data\n",
    "        #print(len(D))\n",
    "        \n",
    "        # Could make a \"get_senders_receivers\" function, that takes\n",
    "        #Â nplanets and returns the arrays. \n",
    "        # Will also have to make one that gets the nodes from the \n",
    "        # masses it is learning\n",
    "        # That will make everything more self contained\n",
    "\n",
    "        graph_dict = { \n",
    "          \"nodes\": nodes_tr,\n",
    "          \"edges\": D, \n",
    "          \"receivers\": receivers_tr, \n",
    "          \"senders\": senders_tr \n",
    "           } \n",
    "    \n",
    "        g = gn.utils_tf.data_dicts_to_graphs_tuple([graph_dict])\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = self(g)\n",
    "            # Compute the loss\n",
    "            loss = tf.keras.losses.mean_squared_error(A, predictions)\n",
    "        \n",
    "        # Compute gradients\n",
    "        trainable_vars = model.graph_network.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients,trainable_vars))\n",
    "\n",
    "        loss_tracker.update_state(loss)\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    def test_step(self,data):\n",
    "        # Unpack the data\n",
    "        D, A = data\n",
    "        \n",
    "        graph_dict = { \n",
    "          \"nodes\": nodes_val,\n",
    "          \"edges\": D, \n",
    "          \"receivers\": receivers_val,\n",
    "          \"senders\": senders_val \n",
    "           } \n",
    "\n",
    "        g = gn.utils_tf.data_dicts_to_graphs_tuple([graph_dict])\n",
    "        \n",
    "        predictions = self(g)\n",
    "\n",
    "        #self.A_pred = tf.reshape(predictions, shape=[-1, self.nplanets, 3])\n",
    "        #loss_test = tf.keras.metrics.MeanAbsoluteError(A, predictions)\n",
    "        #loss_tracker.update_state(loss_test)\n",
    "\n",
    "        #self.losses.append(tf.keras.losses.MeanAbsoluteError)\n",
    "        # Updates the metrics tracking the loss\n",
    "        #self.compiled_loss(A, predictions, regularization_losses=self.losses) \n",
    "    \n",
    "        # Update the metrics.\n",
    "        loss_test.update_state(A, predictions)\n",
    "        \n",
    "        return {\"loss\": loss_test.result()}\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker, loss_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "model = LearnForces()\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 6s 611us/step - loss: 1.1491\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 0.4437\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 576us/step - loss: 0.2279\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 613us/step - loss: 0.1116\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 580us/step - loss: 0.0609\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 555us/step - loss: 0.0371\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 544us/step - loss: 0.0242\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 536us/step - loss: 0.0166\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 627us/step - loss: 0.0123\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 627us/step - loss: 0.0096\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 0.0075\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 653us/step - loss: 0.0057\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 0.0039\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 618us/step - loss: 0.0031\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 635us/step - loss: 0.0026\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 0.0023\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 608us/step - loss: 0.0019\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 822us/step - loss: 0.0017\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 636us/step - loss: 0.0016\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 659us/step - loss: 0.0015\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 664us/step - loss: 0.0014\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 650us/step - loss: 0.0013\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 614us/step - loss: 0.0013\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 556us/step - loss: 0.0013\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 0.0012\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 783us/step - loss: 0.0012\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 0.0011\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 733us/step - loss: 0.0011\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 938us/step - loss: 0.0011\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 744us/step - loss: 0.0011\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 721us/step - loss: 0.0011\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 663us/step - loss: 0.0010\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 0.0010\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 9.9936e-04\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 9.7739e-04\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 602us/step - loss: 9.5103e-04\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 584us/step - loss: 9.3936e-04\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 9.2437e-04\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 9.1292e-04\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 591us/step - loss: 9.0721e-04\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 586us/step - loss: 8.9841e-04\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 587us/step - loss: 8.8642e-04\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 590us/step - loss: 8.7676e-04\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 8.6292e-04\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 642us/step - loss: 8.5094e-04\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 621us/step - loss: 8.4326e-04\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 8.3443e-04\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 711us/step - loss: 8.2161e-04\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 8.0990e-04\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 8.0397e-04\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 7.9164e-04\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 609us/step - loss: 7.8041e-04\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 634us/step - loss: 7.7449e-04\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 638us/step - loss: 7.6850e-04\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 631us/step - loss: 7.6058e-04\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 637us/step - loss: 7.5384e-04\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 629us/step - loss: 7.4567e-04\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 632us/step - loss: 7.3968e-04\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 7.3693e-04\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 627us/step - loss: 7.2351e-04\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 7.1761e-04\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 659us/step - loss: 7.0661e-04\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 634us/step - loss: 6.9706e-04\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 599us/step - loss: 6.8011e-04\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 6.7206e-04\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 622us/step - loss: 6.6394e-04\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 6.6027e-04\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 6.5292e-04\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 612us/step - loss: 6.4826e-04\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 629us/step - loss: 6.4690e-04\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 595us/step - loss: 6.4015e-04\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 616us/step - loss: 6.3217e-04\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 1s 626us/step - loss: 6.3215e-04\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 6.2320e-04\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 635us/step - loss: 6.2292e-04\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 6.1474e-04\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 656us/step - loss: 6.1433e-04\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 643us/step - loss: 6.1078e-04\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 610us/step - loss: 6.0665e-04\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 574us/step - loss: 6.0491e-04\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 548us/step - loss: 6.0032e-04\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 550us/step - loss: 5.9493e-04\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 605us/step - loss: 5.9178e-04\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 611us/step - loss: 5.9240e-04\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 597us/step - loss: 5.9230e-04\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 5.8547e-04\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 617us/step - loss: 5.8156e-04\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 593us/step - loss: 5.8159e-04\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 598us/step - loss: 5.7802e-04\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 603us/step - loss: 5.7413e-04\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 594us/step - loss: 5.7487e-04\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 641us/step - loss: 5.7090e-04\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 596us/step - loss: 5.6832e-04\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 628us/step - loss: 5.6433e-04\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 601us/step - loss: 5.6299e-04\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 5.6316e-04\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 640us/step - loss: 5.6097e-04\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 615us/step - loss: 5.5710e-04\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 637us/step - loss: 5.5507e-04\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 582us/step - loss: 5.5265e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb7f4f1cc50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs = 100, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 317ms/step - loss: 0.0161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.016069306060671806"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = { \n",
    "          \"nodes\": nodes_val,\n",
    "          \"edges\": D_val_flat[0], \n",
    "          \"receivers\": receivers_val, \n",
    "          \"senders\": senders_val \n",
    "           } \n",
    "    \n",
    "g = gn.utils_tf.data_dicts_to_graphs_tuple([graph_dict])\n",
    "out = model(g)\n",
    "ap = tf.reshape(out, shape=[-1, nplanets, 3]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb7f42ea050>,\n",
       " <matplotlib.lines.Line2D at 0x7fb7da919c10>,\n",
       " <matplotlib.lines.Line2D at 0x7fb7d6938750>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ap[:,:,0], ap[:,:,1], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "### To finish the project\n",
    "- Include more planets\n",
    "- Include moons\n",
    "- Learn masses\n",
    "- Extract forces\n",
    "- Add 3D rotations?\n",
    "- Add noise?\n",
    "- Add custom loss function?\n",
    "\n",
    "### To clean the model\n",
    "- Include loss_test in fit \n",
    "- Use it for early stopping? \n",
    "- Make the model calculate the senders and receivers from input shape\n",
    "- Include creating the graph in the model call, so the model takes an array of distances, not a graph? (Check if this makes things slower). See https://www.tensorflow.org/guide/keras/functional\n",
    "- Make a separate script that creates the data\n",
    "- Include some kind of training progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gn_tf2",
   "language": "python",
   "name": "gn_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
