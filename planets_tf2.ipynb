{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to figure out the polar coordinates, otherwise things just do not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import graph_nets as gn\n",
    "import sonnet as snt\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import read_orbits\n",
    "from solar_system_names import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "AU = 149.6e6 * 1000 # Astronomical Unit in meters.\n",
    "DAY = 24*3600. # Day in seconds\n",
    "YEAR = 365.25*DAY #Year\n",
    "delta_time = (2/24.) # 2 hours\n",
    "MSUN = 1.9885e+30\n",
    "MEARTH = 5.9724e+24\n",
    "G = 6.67428e-11/AU**3*MEARTH*DAY**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the tensorflow_graphics package\n",
    "\n",
    "def log10(x):\n",
    "    #numerator = tf.log(x)\n",
    "    #denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n",
    "    #return numerator / denominator\n",
    "    return tf.experimental.numpy.log10(x)\n",
    "\n",
    "def cartesian_to_spherical_coordinates(point_cartesian, eps=None):\n",
    "    \"\"\"Function to transform Cartesian coordinates to spherical coordinates.\n",
    "    This function assumes a right handed coordinate system with `z` pointing up.\n",
    "    When `x` and `y` are both `0`, the function outputs `0` for `phi`. Note that\n",
    "    the function is not smooth when `x = y = 0`.\n",
    "    Note:\n",
    "      In the following, A1 to An are optional batch dimensions.\n",
    "    Args:\n",
    "      point_cartesian: A tensor of shape `[A1, ..., An, 3]`. In the last\n",
    "        dimension, the data follows the `x`, `y`, `z` order.\n",
    "      eps: A small `float`, to be added to the denominator. If left as `None`,\n",
    "        its value is automatically selected using `point_cartesian.dtype`.\n",
    "      name: A name for this op. Defaults to `cartesian_to_spherical_coordinates`.\n",
    "    Returns:\n",
    "      A tensor of shape `[A1, ..., An, 3]`. The last dimensions contains\n",
    "      (`r`,`theta`,`phi`), where `r` is the sphere radius, `theta` is the polar\n",
    "      angle and `phi` is the azimuthal angle.\n",
    "    \"\"\"\n",
    "    #with tf.compat.v1.name_scope(name, \"cartesian_to_spherical_coordinates\",\n",
    "    #                             [point_cartesian]):\n",
    "    #  point_cartesian = tf.convert_to_tensor(value=point_cartesian)\n",
    "\n",
    "    #shape.check_static(\n",
    "    #    tensor=point_cartesian,\n",
    "    #    tensor_name=\"point_cartesian\",\n",
    "    #    has_dim_equals=(-1, 3))\n",
    "\n",
    "    x, y, z = tf.unstack(point_cartesian, axis=-1)\n",
    "    radius = tf.norm(tensor=point_cartesian, axis=-1)\n",
    "    theta = tf.acos(\n",
    "        tf.clip_by_value(tf.divide(z, radius), -1., 1.))\n",
    "    phi = tf.atan2(y, x)\n",
    "    return tf.stack((log10(radius), theta, phi), axis=-1)\n",
    "\n",
    "def spherical_to_cartesian_coordinates(point_spherical, name=None):\n",
    "    \"\"\"Function to transform Cartesian coordinates to spherical coordinates.\n",
    "    Note:\n",
    "      In the following, A1 to An are optional batch dimensions.\n",
    "    Args:\n",
    "      point_spherical: A tensor of shape `[A1, ..., An, 3]`. The last dimension\n",
    "        contains r, theta, and phi that respectively correspond to the radius,\n",
    "        polar angle and azimuthal angle; r must be non-negative.\n",
    "      name: A name for this op. Defaults to 'spherical_to_cartesian_coordinates'.\n",
    "    Raises:\n",
    "      tf.errors.InvalidArgumentError: If r, theta or phi contains out of range\n",
    "      data.\n",
    "    Returns:\n",
    "      A tensor of shape `[A1, ..., An, 3]`, where the last dimension contains the\n",
    "      cartesian coordinates in x,y,z order.\n",
    "    \"\"\"\n",
    "    #with tf.compat.v1.name_scope(name, \"spherical_to_cartesian_coordinates\",\n",
    "    #                           [point_spherical]):\n",
    "    #point_spherical = tf.convert_to_tensor(value=point_spherical)\n",
    "\n",
    "    #shape.check_static(\n",
    "    #    tensor=point_spherical,\n",
    "    #    tensor_name=\"point_spherical\",\n",
    "    #    has_dim_equals=(-1, 3))\n",
    "\n",
    "    logr, theta, phi = tf.unstack(point_spherical, axis=-1)\n",
    "    r = tf.pow(logr, 10)\n",
    "    #r = asserts.assert_all_above(r, 0)\n",
    "    tmp = r * tf.sin(theta)\n",
    "    x = tmp * tf.cos(phi)\n",
    "    y = tmp * tf.sin(phi)\n",
    "    z = r * tf.cos(theta)\n",
    "    return tf.stack((x, y, z), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_newton(x, m1, m2):\n",
    "    return G*m1*m2/np.linalg.norm(x, axis = -1, keepdims=True)**3.*x\n",
    "\n",
    "def reshape_senders_receivers(senders, receivers, batch_size, nplanets, nedges):\n",
    "    ''' Reshape receivers and senders to use in graph'''\n",
    "    x = np.arange(batch_size)\n",
    "    xx = x.reshape(batch_size,1)\n",
    "    y = np.ones(nedges)\n",
    "    z = np.reshape(xx+y-1, batch_size*nedges)*nplanets\n",
    "\n",
    "    senders = np.concatenate([senders]*batch_size) + z\n",
    "    receivers = np.concatenate([receivers]*batch_size) + z\n",
    "    \n",
    "    return senders, receivers\n",
    "\n",
    "def build_rotation_matrix(a,b,g):\n",
    "    A0 = tf.stack([tf.cos(a)*tf.cos(b), tf.sin(a)*tf.cos(b), -tf.sin(b)], \n",
    "                  axis=0)\n",
    "    A1 = tf.stack([tf.cos(a)*tf.sin(b)*tf.sin(g)-tf.sin(a)*tf.cos(g), \n",
    "                   tf.sin(a)*tf.sin(b)*tf.sin(g)+tf.cos(a)*tf.cos(g),\n",
    "                   tf.cos(b)*tf.sin(g)], axis=0)\n",
    "    A2 = tf.stack([tf.cos(a)*tf.sin(b)*tf.cos(g)+tf.sin(a)*tf.sin(g), \n",
    "                   tf.sin(a)*tf.sin(b)*tf.cos(g)-tf.cos(a)*tf.sin(g),\n",
    "                   tf.cos(b)*tf.cos(g)], axis=0)\n",
    "    \n",
    "    return tf.stack((A0, A1, A2), axis=1)\n",
    "\n",
    "def rotate_data(D, A):\n",
    "    # I think the maxes should be 2pi, pi, pi, but going for overkill just in case\n",
    "    alpha = tf.random.uniform([], minval=0, maxval=2*np.pi, dtype=tf.dtypes.float32)\n",
    "    beta = tf.random.uniform([], minval=0, maxval=2*np.pi, dtype=tf.dtypes.float32)\n",
    "    gamma = tf.random.uniform([], minval=0, maxval=2*np.pi, dtype=tf.dtypes.float32)\n",
    "    #print(alpha) #It works! (Different every time)\n",
    "    R = build_rotation_matrix(alpha,beta,gamma)\n",
    "    #D = tf.einsum('ij,jk->ik', D,R)\n",
    "    #A = tf.einsum('ij,jk->ik', A,R)\n",
    "    D = tf.linalg.matmul(D,R)\n",
    "    A = tf.linalg.matmul(A,R)\n",
    "    return D, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables\n",
    "patience = 5\n",
    "d_patience = 0\n",
    "noise_level = 0.05\n",
    "log_every_iterations = 1000\n",
    "num_training_iterations = 200000\n",
    "\n",
    "# Do not change this\n",
    "total_time_traj = 20 #Years\n",
    "num_time_steps_total = int(total_time_traj/delta_time)\n",
    "num_time_steps_tr = 130000 #An orbit for saturn is 129110 steps\n",
    "num_time_steps_sr = 3000\n",
    "num_batches = 1300\n",
    "#num_time_steps_val = int(total_time_traj/delta_time) - num_time_steps_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data in Solar System barycenter reference frame\n",
      "Reading data for mercury\n",
      "Reading data for venus\n",
      "Reading data for earth\n",
      "Reading data for mars\n",
      "Reading data for jupiter\n",
      "Reading data for saturn\n",
      "Finished reading data\n",
      "The data array contains 7 bodies.\n"
     ]
    }
   ],
   "source": [
    "nplanets = 6 #Â Number of planets (not counting the sun)\n",
    "data, masses, names = read_orbits.main(nplanets = nplanets, frame='b', use_moons = False, \n",
    "                                       path='/Users/Pablo/Dropbox/data/orbits/full/')\n",
    "nplanets = len(data[0])\n",
    "nedges = nplanets*(nplanets-1)//2\n",
    "batch_size_tr = num_time_steps_tr//num_batches\n",
    "\n",
    "# Get the acceleration\n",
    "A = data[1:,:,3:] - data[:-1,:,3:]\n",
    "data[:-1, :, 3:] = A/delta_time \n",
    "data = data[:-1]\n",
    "\n",
    "masses/=MEARTH#/1000000\n",
    "\n",
    "# Split into training and validation\n",
    "data_tr = data[:num_time_steps_tr]\n",
    "data_val = data[num_time_steps_tr:]\n",
    "\n",
    "num_time_steps_val = len(data_val)\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data_tr)\n",
    "np.random.shuffle(data_val)\n",
    "\n",
    "D_tr_np = np.empty([len(data_tr), nedges, 3])\n",
    "D_val_np = np.empty([len(data_val), nedges, 3])\n",
    "F_val = np.empty([len(data_val), nedges, 3])\n",
    "k=0\n",
    "names_edges = []\n",
    "senders, receivers = [], []\n",
    "for i in range(nplanets):\n",
    "    for j in range(nplanets):\n",
    "        if i > j:\n",
    "            d_tr = data_tr[:,j,:3] - data_tr[:,i,:3]\n",
    "            d_val = data_val[:,j,:3] - data_val[:,i,:3]\n",
    "            D_tr_np[:,k,:] = d_tr#cartesian_to_spherical_coordinates(d_tr)\n",
    "            D_val_np[:,k,:] = d_val #cartesian_to_spherical_coordinates(d_val)\n",
    "            F_val[:,k,:] = force_newton(d_val, masses[i], masses[j]) #cartesian_to_spherical_coordinates(d_val)\n",
    "            names_edges.append(names[j] + ' - ' + names[i])\n",
    "            \n",
    "            k+=1 \n",
    "            receivers.append(i)\n",
    "            senders.append(j)\n",
    "\n",
    "A_tr = data_tr[:,:,3:]\n",
    "A_val = data_val[:,:,3:]\n",
    "A_norm =np.std(A_tr) \n",
    "\n",
    "A_norm_dim= tf.reduce_mean(tf.norm(A_tr/A_norm, axis = -1, keepdims = False), axis = 0)\n",
    "A_norm_tr = tf.convert_to_tensor([A_norm_dim]*batch_size_tr, dtype=\"float32\")\n",
    "A_norm_tr = tf.reshape(A_norm_tr, shape = [batch_size_tr*nplanets, 1])\n",
    "A_norm_val = tf.convert_to_tensor([A_norm_dim]*num_time_steps_val, dtype=\"float32\")\n",
    "A_norm_val = tf.reshape(A_norm_val, shape = [num_time_steps_val*nplanets, 1])\n",
    "\n",
    "D_tr_flat = np.reshape(D_tr_np, [num_time_steps_tr*nedges, 3])\n",
    "D_val_flat = np.reshape(D_val_np,[1, num_time_steps_val*nedges, 3])\n",
    "\n",
    "A_tr_flat = np.reshape(A_tr/A_norm, [num_time_steps_tr*nplanets, 3])\n",
    "A_val_flat = np.reshape(A_val/A_norm, [1, num_time_steps_val*nplanets, 3])\n",
    "\n",
    "D_tr = tf.convert_to_tensor(D_tr_flat, dtype=\"float32\")\n",
    "A_tr = tf.convert_to_tensor(A_tr_flat, dtype=\"float32\")\n",
    "D_tr_batches = tf.split(D_tr,  num_batches)\n",
    "A_tr_batches = tf.split(A_tr,  num_batches)\n",
    "\n",
    "D_val = tf.convert_to_tensor(D_val_flat, dtype=\"float32\")\n",
    "A_val = tf.convert_to_tensor(A_val_flat, dtype=\"float32\")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (D_tr_batches, A_tr_batches))#.batch(batch_size_tr)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (D_val, A_val))\n",
    "\n",
    "#D_val = tf.convert_to_tensor(D_val[:,:,:], dtype=\"float32\")\n",
    "#A_val = tf.convert_to_tensor(A_val/A_norm, dtype=\"float32\")\n",
    "\n",
    "#D_val_flat = tf.reshape(D_val, shape = [num_time_steps_val*nedges, 3])\n",
    "#A_val_flat = tf.reshape(A_val/A_norm, shape = [num_time_steps_val*nedges, 3])\n",
    "\n",
    "masses_tf = tf.convert_to_tensor(masses, dtype=\"float32\")\n",
    "\n",
    "nodes_tr = np.concatenate([np.log10(masses)]*batch_size_tr)[:,np.newaxis]\n",
    "nodes_val = np.concatenate([np.log10(masses)]*num_time_steps_val)[:,np.newaxis]\n",
    "nodes_sr = np.concatenate([np.log10(masses)]*num_time_steps_sr)[:,np.newaxis]\n",
    "\n",
    "senders_tr, receivers_tr = reshape_senders_receivers(senders, receivers, batch_size_tr, nplanets, nedges)\n",
    "senders_val, receivers_val = reshape_senders_receivers(senders, receivers, num_time_steps_val, nplanets, nedges)\n",
    "senders_sr, receivers_sr = reshape_senders_receivers(senders, receivers, num_time_steps_sr, nplanets, nedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_weighted_error(y_true, y_pred, norm):\n",
    "    x = (y_pred - y_true)/tf.abs(y_true) #/norm*tf.reduce_mean(norm)\n",
    "\n",
    "    return tf.reduce_mean(tf.reduce_sum(x**2, axis = -1))\n",
    "    \n",
    "class MeanWeightedError(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"mean_weighted_error\", **kwargs):\n",
    "        super(MeanWeightedError, self).__init__(name=name, **kwargs)\n",
    "        self.mwe = self.add_weight(name=\"mwe\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, norm):\n",
    "        self.mwe.assign_add(mean_weighted_error(y_true, y_pred, norm))\n",
    "\n",
    "    def result(self):\n",
    "        return self.mwe\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.mwe.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nplanets, _, ntime = X.shape\n",
    "loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "#loss_test = tf.keras.metrics.MeanAbsoluteError(name='loss_test')\n",
    "loss_test = MeanWeightedError(name='loss_test')\n",
    "\n",
    "class LearnForces(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LearnForces, self).__init__()\n",
    "        #self.test_loss_metric = tf.keras.metrics.MeanAbsoluteError(name='test_loss')\n",
    "        \n",
    "        logm_init = tf.random_normal_initializer(mean=0.0, stddev=5.0)\n",
    "        self.logm = tf.Variable(\n",
    "            initial_value=logm_init(shape=(nplanets,), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        self.graph_network = gn.blocks.EdgeBlock(\n",
    "            edge_model_fn=lambda: snt.nets.MLP([32, 32, 3]),\n",
    "            #edge_model_fn=lambda: snt.nets.MLP([128,128,128,128, 3]),\n",
    "            use_edges = True,\n",
    "            use_receiver_nodes = True,\n",
    "            use_sender_nodes = True,\n",
    "            use_globals = False,\n",
    "        )\n",
    "\n",
    "\n",
    "    def sum_forces(self, graph):\n",
    "        b1_tr = gn.blocks.ReceivedEdgesToNodesAggregator(reducer = tf.math.unsorted_segment_sum)(graph)\n",
    "        b2_tr = gn.blocks.SentEdgesToNodesAggregator(reducer = tf.math.unsorted_segment_sum)(graph)\n",
    "        summed_forces = b1_tr-b2_tr\n",
    "        return summed_forces\n",
    "            \n",
    "    def get_acceleration(self, forces, graph):\n",
    "        acceleration_tr = tf.divide(forces, tf.pow(10.,graph.nodes))\n",
    "        return acceleration_tr\n",
    "        #output_ops_tr = tf.reshape(acceleration_tr, shape=[self.ntime, self.nplanets, 3])\n",
    "        #return output_ops_tr\n",
    "        \n",
    "    def call(self, D, extract = False):\n",
    "        #self.ntime = len(g.nodes)//nplanets\n",
    "        ntime = len(D)//nedges\n",
    "        #nodes_g = np.concatenate([np.log10(masses_tf)]*ntime)[:,np.newaxis]\n",
    "        nodes_g = tf.concat([self.logm]*ntime, axis = 0)\n",
    "        nodes_g = tf.expand_dims(nodes_g, 1)\n",
    "        senders_g, receivers_g = reshape_senders_receivers(senders, receivers, ntime, nplanets, nedges)\n",
    "        \n",
    "        # Create graph\n",
    "        graph_dict = { \n",
    "          \"nodes\": nodes_g,\n",
    "          \"edges\": cartesian_to_spherical_coordinates(D), \n",
    "          \"receivers\": receivers_g, \n",
    "          \"senders\": senders_g \n",
    "           } \n",
    "    \n",
    "        g = gn.utils_tf.data_dicts_to_graphs_tuple([graph_dict])\n",
    "        \n",
    "        g = self.graph_network(g)\n",
    "        g = g.replace(\n",
    "            edges = spherical_to_cartesian_coordinates(g.edges))\n",
    "        f = self.sum_forces(g)\n",
    "        a = self.get_acceleration(f, g)\n",
    "        if extract == True: \n",
    "            f = tf.reshape(g.edges, shape=[-1, nedges, 3]).numpy()\n",
    "            a = tf.reshape(a, shape=[-1, nplanets, 3]).numpy()\n",
    "            return a, f\n",
    "        else: \n",
    "            return a\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        #if isinstance(data, tuple):\n",
    "        #    data = data[0]\n",
    "        # Unpack the data\n",
    "        D, A = data\n",
    "        \n",
    "        D_rot, A_rot = rotate_data(D, A)\n",
    "        \n",
    "        D_noise = tf.random.normal(tf.shape(D), 0, noise_level, tf.float32)\n",
    "        D_rot = D_rot + D_noise\n",
    "                                   \n",
    "        # Could make a \"get_senders_receivers\" function, that takes\n",
    "        #Â nplanets and returns the arrays. \n",
    "        # Will also have to make one that gets the nodes from the \n",
    "        # masses it is learning\n",
    "        # That will make everything more self contained\n",
    "\n",
    "        # Randomly 3D rotate the data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = self(D_rot)\n",
    "            # Compute the loss\n",
    "            loss = mean_weighted_error(A_rot, predictions, A_norm_tr)\n",
    "        \n",
    "        # Compute gradients\n",
    "        # Trainable variables are the masses and the MLP layers \n",
    "        trainable_vars = self.trainable_variables+ list(self.graph_network.trainable_variables)\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients,trainable_vars))\n",
    "\n",
    "        loss_tracker.update_state(loss)\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    def test_step(self,data):\n",
    "        # Unpack the data\n",
    "        D, A = data\n",
    "        \n",
    "        predictions = self(D)\n",
    "\n",
    "        #self.A_pred = tf.reshape(predictions, shape=[-1, self.nplanets, 3])\n",
    "        #loss_test = tf.keras.metrics.MeanAbsoluteError(A, predictions)\n",
    "        #loss_tracker.update_state(loss_test)\n",
    "\n",
    "        #self.losses.append(tf.keras.losses.MeanAbsoluteError)\n",
    "        # Updates the metrics tracking the loss\n",
    "        #self.compiled_loss(A, predictions, regularization_losses=self.losses) \n",
    "    \n",
    "        # Update the metrics.\n",
    "        #loss_test.update_state(A, predictions)\n",
    "        loss_test.update_state(A, predictions, A_norm_val)\n",
    "        \n",
    "        return {\"loss\": loss_test.result()}\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker, loss_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = './saved_models/test'\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
    "                                            verbose = 1,\n",
    "                                            patience=10, \n",
    "                                            restore_best_weights=False)\n",
    "#Â Restore best weights not working, but found way around using checkpoint\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                 save_weights_only=False,\n",
    "                                                save_best_only=True,\n",
    "                                                 verbose=0)\n",
    "model = LearnForces()\n",
    "\n",
    "#model.compile(run_eagerly=True)\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, \n",
    "          epochs = 1000, \n",
    "          verbose=2,\n",
    "          #callbacks=[early_stopping], \n",
    "          callbacks=[early_stopping, checkpoint], \n",
    "          validation_data=test_ds\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.graph_network.trainable_variables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap ,fp = model(D_val_flat[0], extract = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = math.ceil(nplanets/4)\n",
    "fig, ax = plt.subplots(nrows, 4, figsize = (16, 4*nrows))\n",
    "for i in range(nplanets):\n",
    "    ax[i//4, i%4].set_title(names[i], fontsize = 16)\n",
    "    ax[i//4, i%4].plot(data_val[:,i,3]/A_norm, data_val[:,i,4]/A_norm, '.')\n",
    "    ax[i//4, i%4].plot(ap[:,i,0], ap[:,i,1], '.')\n",
    "\n",
    "#plt.savefig('/Users/Pablo/Desktop/acc_learnedmasses.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = math.ceil(nedges/4)\n",
    "fig, ax = plt.subplots(nrows, 4, figsize = (16, 4*nrows))\n",
    "for i in range(nedges):\n",
    "    ax[i//4, i%4].set_title(names_edges[i])\n",
    "    #ax[i//4, i%4].plot(F_val[:,i,0], F_val[:,i,1], '.')\n",
    "    ax[i//4, i%4].plot(fp[:,i,0]*A_norm, fp[:,i,1]*A_norm, '.')\n",
    "#plt.savefig('/Users/Pablo/Desktop/forces_learnedmasses.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**model.logm.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = tf.reshape(D_val, [-1, nedges, 3])\n",
    "dv, fp = rotate_data(dv, fp)\n",
    "F_pred_sr = np.empty([num_time_steps_sr, nedges, 3])\n",
    "D_val_sr = np.empty([num_time_steps_sr, nedges, 3])\n",
    "for i in range(num_time_steps_sr):\n",
    "    Dv_temp, Fp_temp = rotate_data(dv[i], fp[i])\n",
    "    F_pred_sr[i] = Fp_temp\n",
    "    D_val_sr[i] = Dv_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "indices = [0,1,3,6,10,15,21]\n",
    "X = np.zeros([(nplanets-1)*num_time_steps_sr,5])\n",
    "F = np.zeros([(nplanets-1)*num_time_steps_sr,3])\n",
    "F_norm = np.mean(F_pred_sr)\n",
    "k=0\n",
    "for i in range(nplanets):\n",
    "    for j in range(nplanets):\n",
    "        if ((i>j) and (j==0)):\n",
    "            print(i,j,k, indices[i])\n",
    "            X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,0] = masses[i]\n",
    "           #X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,1] = masses[j]\n",
    "            X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,1:4] = D_val_sr[:,indices[k],:]\n",
    "            X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,4] = np.linalg.norm(D_val_sr[:,indices[k],:], axis = -1)#**3\n",
    "            F[k*num_time_steps_sr:(k+1)*num_time_steps_sr,:] = F_pred_sr[:,indices[k],:]#/F_norm #works better with\n",
    "            k+=1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros([nedges*num_time_steps_sr,6])\n",
    "F = np.zeros([nedges*num_time_steps_sr,3])\n",
    "F_norm = np.mean(F_pred_sr)\n",
    "k=0\n",
    "masses_learned = 10**model.logm.numpy()\n",
    "for i in range(nplanets):\n",
    "    for j in range(nplanets):\n",
    "        if i>j:\n",
    "            #X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,0] = masses[i]\n",
    "            #X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,1] = masses[j]\n",
    "            X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,0] = masses_learned[i]\n",
    "            X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,1] = masses_learned[j]\n",
    "            X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,2:5] = D_val_sr[:,k,:]\n",
    "            X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,5] = np.linalg.norm(D_val_sr[:,k,:], axis = -1)#**3\n",
    "            F[k*num_time_steps_sr:(k+1)*num_time_steps_sr,:] = F_pred_sr[:,k,:]#/F_norm #works better with\n",
    "            k+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_val_sr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysr import pysr\n",
    "# Learn equations\n",
    "equations = []\n",
    "for i in range(1):\n",
    "    equation = pysr(X[:,:], F[:,i]/np.std(F[:,i]), niterations=20,\n",
    "            #maxsize = 100,\n",
    "            populations = 16,\n",
    "            #variable_names = ['m0', 'm1', 'x', 'y', 'z', 'r'],\n",
    "            binary_operators=[\"mult\", \"div\"],\n",
    "            unary_operators=[\"square\", \"cube\"])\n",
    "    equations.append(equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asCartesian(rthetaphi):\n",
    "    #takes list rthetaphi (single coord)\n",
    "    r       = rthetaphi[0]\n",
    "    theta   = rthetaphi[1]* pi/180 # to radian\n",
    "    phi     = rthetaphi[2]* pi/180\n",
    "    x = r * sin( theta ) * cos( phi )\n",
    "    y = r * sin( theta ) * sin( phi )\n",
    "    z = r * cos( theta )\n",
    "    return [x,y,z]\n",
    "\n",
    "def asSpherical(xyz):\n",
    "    #takes list xyz (single coord)\n",
    "    x       = xyz[0]\n",
    "    y       = xyz[1]\n",
    "    z       = xyz[2]\n",
    "    r       =  sqrt(x*x + y*y + z*z)\n",
    "    theta   =  acos(z/r)*180/ pi #to degrees\n",
    "    phi     =  atan2(y,x)*180/ pi\n",
    "    return [r,theta,phi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros([nedges*num_time_steps_sr,6])\n",
    "F = np.zeros([nedges*num_time_steps_sr,3])\n",
    "F_norm = np.mean(F_pred_sr)\n",
    "k=0\n",
    "for i in range(nplanets):\n",
    "    for j in range(nplanets):\n",
    "        if i>j:\n",
    "            X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,0] = np.log10(masses[i])\n",
    "            X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,1] = np.log10(masses[j])\n",
    "            X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,2:5] = asSphericalD_val_sr[:,k,:]\n",
    "            X[k*num_time_steps_sr:(k+1)*num_time_steps_sr,5] = np.linalg.norm(D_val_sr[:,k,:], axis = -1)#**3\n",
    "            F[k*num_time_steps_sr:(k+1)*num_time_steps_sr,:] = F_pred_sr[:,k,:]#/F_norm #works better with\n",
    "            k+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "### To finish the project\n",
    "- Include moons\n",
    "- Learn masses\n",
    "- Use barycenter data directly\n",
    "\n",
    "### To clean the model\n",
    "- Clean mean_weighted_error implementation\n",
    "- Include loss_test in fit \n",
    "- Use it for early stopping? \n",
    "- Make the model calculate the senders and receivers from input shape\n",
    "- Include creating the graph in the model call, so the model takes an array of distances, not a graph? (Check if this makes things slower). See https://www.tensorflow.org/guide/keras/functional\n",
    "- Make a separate script that creates the data\n",
    "- Include some kind of training progress bar\n",
    "- Improve plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For next meeting\n",
    "- So far, with the moons, even with the true masses given, the MLP struggles with learning. It gets stuck sending all edges involving the moons to zero. Suggestions?\n",
    "- When I try to do symbolic regression with all edges (not just those involving the sun) PySR often struggles to find the correct formula, instead it gets stuck in things like m1*x or m0*x/r, even though when it does find the correct formula, it has a massively smaller MSE. Are there any settings I can give PySR to help it? It does work if I give it $r^3$ as a parameter, but that feels like cheating\n",
    "- It keeps learning Saturn wrong, but I have no idea why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "- The graphnets tensorflow 2 installation\n",
    "- pysr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[:100,5,0], data[:100,5,1], '.')\n",
    "plt.plot(data[:100,6,0], data[:100,6,1], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(senders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(senders), batch_size_tr, nplanets, nedges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gn_tf2",
   "language": "python",
   "name": "gn_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
